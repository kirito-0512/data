7193,
Commutator,
In mathematics, the commutator gives an indication of the extent to which a certain binary operation fails to be commutative. There are different definitions used in group theory and ring theory.
The commutator of two elements, g and h, of a group G, is the element
This element is equal to the group's identity if and only if g and h commute (from the definition gh = hg [g, h], being [g, h]  equal to the identity if and only if gh = hg).
The set of all commutators of a group is not in general closed under the group operation, but the subgroup of G generated by all commutators is closed and is called the derived group or the commutator subgroup of G. Commutators are used to define nilpotent and solvable groups and the largest abelian quotient group.
The definition of the commutator above is used throughout this article, but many other group theorists define the commutator as
Commutator identities are an important tool in group theory.[3] The expression ax denotes the conjugate of a by x, defined as x−1ax.
Identity (5) is also known as the Hall–Witt identity, after Philip Hall and Ernst Witt.  It is a group-theoretic analogue of the Jacobi identity for the ring-theoretic commutator (see next section).
N.B., the above definition of the conjugate of a by x is used by some group theorists.[4]  Many other group theorists define the conjugate of a by x as xax−1.[5]  This is often written 
x
a
{\displaystyle {}^{x}a}
.  Similar identities hold for these conventions.
Many identities are used that are true modulo certain subgroups.  These can be particularly useful in the study of solvable groups and nilpotent groups.  For instance, in any group, second powers behave well:
If the derived subgroup is central, then
Rings often do not support division. Thus, the commutator of two elements a and b of a ring (or any associative algebra) is defined differently by
The commutator is zero if and only if a and b commute. In linear algebra, if two endomorphisms of a space are represented by commuting matrices in terms of one basis, then they are so represented in terms of every basis. By using the commutator as a Lie bracket, every associative algebra can be turned into a Lie algebra.
The anticommutator of two elements a and b of a ring or associative algebra is defined by
Sometimes 
[
a
,
b
]
+
{\displaystyle [a,b]_{+}}
 is used to denote anticommutator, while 
[
a
,
b
]
−
{\displaystyle [a,b]_{-}}
 is then used for commutator.[6] The anticommutator is used less often, but can be used to define Clifford algebras and Jordan algebras and in the derivation of the Dirac equation in particle physics.
The commutator of two operators acting on a Hilbert space is a central concept in quantum mechanics, since it quantifies how well the two observables described by these operators can be measured simultaneously. The uncertainty principle is ultimately a theorem about such commutators, by virtue of the Robertson–Schrödinger relation.[7] In phase space, equivalent commutators of function star-products are called Moyal brackets and are completely isomorphic to the Hilbert space commutator structures mentioned.
The commutator has the following properties:
Relation (3) is called anticommutativity, while (4) is the Jacobi identity.
If A is a fixed element of a ring R, identity (1) can be interpreted as a Leibniz rule for the map 
ad
A
:
R
→
R
{\displaystyle \operatorname {ad} _{A}:R\rightarrow R}
 given by 
ad
A
⁡
(
B
)
=
[
A
,
B
]
{\displaystyle \operatorname {ad} _{A}(B)=[A,B]}
.  In other words, the map adA defines a derivation on the ring R.  Identities (2), (3) represent Leibniz rules for more than two factors, and are valid for any derivation.  Identities (4)–(6) can also be interpreted as Leibniz rules. Identities (7), (8) express Z-bilinearity.
From identity (9), one finds that the commutator of integer powers of ring elements is:
[
A
N
,
B
M
]
=
∑
n
=
0
N
−
1
∑
m
=
0
M
−
1
A
n
B
m
[
A
,
B
]
A
N
−
n
−
1
B
M
−
m
−
1
{\displaystyle [A^{N},B^{M}]=\sum _{n=0}^{N-1}\sum _{m=0}^{M-1}A^{n}B^{m}[A,B]A^{N-n-1}B^{M-m-1}}
Some of the above identities can be extended to the anticommutator using the above ± subscript notation.[8]
For example:
Consider a ring or algebra in which the exponential 
e
A
=
exp
⁡
(
A
)
=
1
+
A
+
1
2
!
A
2
+
⋯
{\displaystyle e^{A}=\exp(A)=1+A+{\tfrac {1}{2!}}A^{2}+\cdots }
 can be meaningfully defined, such as a Banach algebra or a ring of formal power series.
In such a ring, Hadamard's lemma applied to nested commutators gives: 
e
A
B
e
−
A
 
=
 
B
+
[
A
,
B
]
+
1
2
!
[
A
,
[
A
,
B
]
]
+
1
3
!
[
A
,
[
A
,
[
A
,
B
]
]
]
+
⋯
 
=
 
e
ad
A
(
B
)
.
{\textstyle e^{A}Be^{-A}\ =\ B+[A,B]+{\frac {1}{2!}}[A,[A,B]]+{\frac {1}{3!}}[A,[A,[A,B]]]+\cdots \ =\ e^{\operatorname {ad} _{A}}(B).}
 (For the last expression, see Adjoint derivation below.) This formula underlies the Baker–Campbell–Hausdorff expansion of log(exp(A) exp(B)).
A similar expansion expresses the group commutator of expressions 
e
A
{\displaystyle e^{A}}
 (analogous to elements of a Lie group) in terms of a series of nested commutators (Lie brackets),
e
A
e
B
e
−
A
e
−
B
=
exp
(
[
A
,
B
]
+
1
2
!
[
A
+
B
,
[
A
,
B
]
]
+
1
3
!
(
1
2
[
A
,
[
B
,
[
B
,
A
]
]
]
+
[
A
+
B
,
[
A
+
B
,
[
A
,
B
]
]
]
)
+
⋯
)
.
{\displaystyle e^{A}e^{B}e^{-A}e^{-B}=\exp \!\left([A,B]+{\frac {1}{2!}}[A{+}B,[A,B]]+{\frac {1}{3!}}\left({\frac {1}{2}}[A,[B,[B,A]]]+[A{+}B,[A{+}B,[A,B]]]\right)+\cdots \right).}
When dealing with graded algebras, the commutator is usually replaced by the graded commutator, defined in homogeneous components as 
Especially if one deals with multiple commutators in a ring R, another notation turns out to be useful. For an element 
x
∈
R
{\displaystyle x\in R}
, we define the adjoint mapping 
a
d
x
:
R
→
R
{\displaystyle \mathrm {ad} _{x}:R\to R}
 by:
This mapping is a derivation on the ring R: 
By the Jacobi identity, it is also a derivation over the commutation operation: 
Composing such mappings, we get for example 
ad
x
⁡
ad
y
⁡
(
z
)
=
[
x
,
[
y
,
z
]
]
{\displaystyle \operatorname {ad} _{x}\operatorname {ad} _{y}(z)=[x,[y,z]\,]}
 and 
ad
x
2
(
z
)
 
=
 
ad
x
(
ad
x
(
z
)
)
 
=
 
[
x
,
[
x
,
z
]
]
.
{\displaystyle \operatorname {ad} _{x}^{2}\!(z)\ =\ \operatorname {ad} _{x}\!(\operatorname {ad} _{x}\!(z))\ =\ [x,[x,z]\,].}
 We may consider 
a
d
{\displaystyle \mathrm {ad} }
 itself as a mapping, 
a
d
:
R
→
E
n
d
(
R
)
{\displaystyle \mathrm {ad} :R\to \mathrm {End} (R)}
, where 
E
n
d
(
R
)
{\displaystyle \mathrm {End} (R)}
 is the ring of mappings from R to itself with composition as the multiplication operation. Then 
a
d
{\displaystyle \mathrm {ad} }
 is a Lie algebra homomorphism, preserving the commutator:
By contrast, it is not always a ring homomorphism: usually 
ad
x
y
≠
ad
x
⁡
ad
y
{\displaystyle \operatorname {ad} _{xy}\,\neq \,\operatorname {ad} _{x}\operatorname {ad} _{y}}
.
The general Leibniz rule, expanding repeated derivatives of a product, can be written abstractly using the adjoint representation:
Replacing x by the differentiation operator 
∂
{\displaystyle \partial }
, and y by the multiplication operator 
m
f
:
g
↦
f
g
{\displaystyle m_{f}:g\mapsto fg}
, we get 
ad
⁡
(
∂
)
(
m
f
)
=
m
∂
(
f
)
{\displaystyle \operatorname {ad} (\partial )(m_{f})=m_{\partial (f)}}
, and applying both sides to a function g, the identity becomes the usual Leibniz rule for the n-th derivative 
∂
n
(
f
g
)
{\displaystyle \partial ^{n}\!(fg)}
.
