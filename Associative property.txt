1335,
Associative property,
In mathematics, the associative property[1] is a property of some binary operations, which means that rearranging the parentheses in an expression will not change the result. In propositional logic, associativity is a valid rule of replacement for expressions in logical proofs.
Within an expression containing two or more occurrences in a row of the same associative operator, the order in which the operations are performed does not matter as long as the sequence of the operands is not changed. That is (after rewriting the expression with parentheses and in infix notation if necessary), rearranging the parentheses in such an expression will not change its value. Consider the following equations:
(
2
+
3
)
+
4
=
2
+
(
3
+
4
)
=
9
2
×
(
3
×
4
)
=
(
2
×
3
)
×
4
=
24.
{\displaystyle {\begin{aligned}(2+3)+4&=2+(3+4)=9\,\\2\times (3\times 4)&=(2\times 3)\times 4=24.\end{aligned}}}
Even though the parentheses were rearranged on each line, the values of the expressions were not altered. Since this holds true when performing addition and multiplication on any real numbers, it can be said that "addition and multiplication of real numbers are associative operations".
Associativity is not the same as commutativity, which addresses whether the order of two operands affects the result. For example, the order does not matter in the multiplication of real numbers, that is, a × b = b × a, so we say that the multiplication of real numbers is a commutative operation. However, operations such as function composition and matrix multiplication are associative, but (generally) not commutative.
Associative operations are abundant in mathematics; in fact, many algebraic structures (such as semigroups and categories) explicitly require their binary operations to be associative.
However, many important and interesting operations are non-associative; some examples include subtraction, exponentiation, and the vector cross product. In contrast to the theoretical properties of real numbers, the addition of floating point numbers in computer science is not associative, and the choice of how to associate an expression can have a significant effect on rounding error.
Formally, a binary operation ∗ on a set S is called associative if it satisfies the associative law:
Here, ∗ is used to replace the symbol of the operation, which may be any symbol, and even the absence of symbol (juxtaposition) as for multiplication.
The associative law can also be expressed in functional notation thus: f(f(x, y), z) = f(x, f(y, z)).
If a binary operation is associative, repeated application of the operation produces the same result regardless of how valid pairs of parentheses are inserted in the expression.[2] This is called the generalized associative law. For instance, a product of four elements may be written, without changing the order of the factors, in five possible ways:
If the product operation is associative, the generalized associative law says that all these expressions will yield the same result. So unless the expression with omitted parentheses already has a different meaning (see below), the parentheses can be considered unnecessary and "the" product can be written unambiguously as
As the number of elements increases, the number of possible ways to insert parentheses grows quickly, but they remain unnecessary for disambiguation.
An example where this does not work is the logical biconditional ↔. It is associative; thus, A ↔ (B ↔ C) is equivalent to (A ↔ B) ↔ C, but A ↔ B ↔ C most commonly means (A ↔ B) and (B ↔ C), which is not equivalent.
Some examples of associative operations include the following.
(
x
+
y
)
+
z
=
x
+
(
y
+
z
)
=
x
+
y
+
z
(
x
y
)
z
=
x
(
y
z
)
=
x
y
z
 
 
}
for all 
x
,
y
,
z
∈
R
.
{\displaystyle \left.{\begin{matrix}(x+y)+z=x+(y+z)=x+y+z\quad \\(x\,y)z=x(y\,z)=x\,y\,z\qquad \qquad \qquad \quad \ \ \,\end{matrix}}\right\}{\mbox{for all }}x,y,z\in \mathbb {R} .}
In standard truth-functional propositional logic, association,[4][5] or associativity[6] are two valid rules of replacement. The rules allow one to move parentheses in logical expressions in logical proofs. The rules (using logical connectives  notation) are:
(
P
∨
(
Q
∨
R
)
)
⇔
(
(
P
∨
Q
)
∨
R
)
{\displaystyle (P\lor (Q\lor R))\Leftrightarrow ((P\lor Q)\lor R)}
and
(
P
∧
(
Q
∧
R
)
)
⇔
(
(
P
∧
Q
)
∧
R
)
,
{\displaystyle (P\land (Q\land R))\Leftrightarrow ((P\land Q)\land R),}
where "
⇔
{\displaystyle \Leftrightarrow }
" is a metalogical symbol representing "can be replaced in a proof with".
Associativity is a property of some logical connectives of truth-functional propositional logic. The following logical equivalences demonstrate that associativity is a property of particular connectives. The following (and their converses, since ↔ is commutative) are truth-functional tautologies.[citation needed]
Joint denial is an example of a truth functional connective that is not associative.
A binary operation 
∗
{\displaystyle *}
 on a set S that does not satisfy the associative law is called non-associative. Symbolically,
(
x
∗
y
)
∗
z
≠
x
∗
(
y
∗
z
)
for some 
x
,
y
,
z
∈
S
.
{\displaystyle (x*y)*z\neq x*(y*z)\qquad {\mbox{for some }}x,y,z\in S.}
For such an operation the order of evaluation does matter. For example:
Also although addition is associative for finite sums, it is not associative inside infinite sums (series). For example,
(
1
+
−
1
)
+
(
1
+
−
1
)
+
(
1
+
−
1
)
+
(
1
+
−
1
)
+
(
1
+
−
1
)
+
(
1
+
−
1
)
+
⋯
=
0
{\displaystyle (1+-1)+(1+-1)+(1+-1)+(1+-1)+(1+-1)+(1+-1)+\dots =0}
whereas
1
+
(
−
1
+
1
)
+
(
−
1
+
1
)
+
(
−
1
+
1
)
+
(
−
1
+
1
)
+
(
−
1
+
1
)
+
(
−
1
+
1
)
+
⋯
=
1.
{\displaystyle 1+(-1+1)+(-1+1)+(-1+1)+(-1+1)+(-1+1)+(-1+1)+\dots =1.}
Some non-associative operations are fundamental in mathematics. They appear often as the multiplication in structures called non-associative algebras, which have also an addition and a scalar multiplication. Examples are the octonions and Lie algebras. In Lie algebras, the multiplication satisfies Jacobi identity instead of the associative law; this allows abstracting the algebraic nature of infinitesimal transformations.
Other examples are quasigroup, quasifield, non-associative ring, and commutative non-associative magmas.
In mathematics, addition and multiplication of real numbers is associative.  By contrast, in computer science, the addition and multiplication of floating point numbers is not associative, as rounding errors are introduced when dissimilar-sized values are joined together.[7]
To illustrate this, consider a floating point representation with a 4-bit mantissa:
Even though most computers compute with 24 or 53 bits of mantissa,[8] this is an important source of rounding error, and approaches such as the Kahan summation algorithm are ways to minimise the errors. It can be especially problematic in parallel computing.[9][10]
In general, parentheses must be used to indicate the order of evaluation if a non-associative operation appears more than once in an expression (unless the notation specifies the order in another way, like 
2
3
/
4
{\displaystyle {\dfrac {2}{3/4}}}
). However, mathematicians agree on a particular order of evaluation for several common non-associative operations. This is simply a notational convention to avoid parentheses.
A left-associative operation is a non-associative operation that is conventionally evaluated from left to right, i.e.,
a
∗
b
∗
c
=
(
a
∗
b
)
∗
c
a
∗
b
∗
c
∗
d
=
(
(
a
∗
b
)
∗
c
)
∗
d
a
∗
b
∗
c
∗
d
∗
e
=
(
(
(
a
∗
b
)
∗
c
)
∗
d
)
∗
e
etc.
}
for all 
a
,
b
,
c
,
d
,
e
∈
S
{\displaystyle \left.{\begin{array}{l}a*b*c=(a*b)*c\\a*b*c*d=((a*b)*c)*d\\a*b*c*d*e=(((a*b)*c)*d)*e\quad \\{\mbox{etc.}}\end{array}}\right\}{\mbox{for all }}a,b,c,d,e\in S}
while a right-associative operation is conventionally evaluated from right to left:
x
∗
y
∗
z
=
x
∗
(
y
∗
z
)
w
∗
x
∗
y
∗
z
=
w
∗
(
x
∗
(
y
∗
z
)
)
v
∗
w
∗
x
∗
y
∗
z
=
v
∗
(
w
∗
(
x
∗
(
y
∗
z
)
)
)
etc.
}
for all 
z
,
y
,
x
,
w
,
v
∈
S
{\displaystyle \left.{\begin{array}{l}x*y*z=x*(y*z)\\w*x*y*z=w*(x*(y*z))\quad \\v*w*x*y*z=v*(w*(x*(y*z)))\quad \\{\mbox{etc.}}\end{array}}\right\}{\mbox{for all }}z,y,x,w,v\in S}
Both left-associative and right-associative operations occur. Left-associative operations include the following:
This notation can be motivated by the currying isomorphism, which enables partial application.
Right-associative operations include the following:
Exponentiation is commonly used with brackets or right-associatively because a repeated left-associative exponentiation operation is of little use. Repeated powers would mostly be rewritten with multiplication:Formatted correctly, the superscript inherently behaves as a set of parentheses; e.g. in the expression 
2
x
+
3
{\displaystyle 2^{x+3}}
 the addition is performed before the exponentiation despite there being no explicit parentheses 
2
(
x
+
3
)
{\displaystyle 2^{(x+3)}}
 wrapped around it. Thus given an expression such as 
x
y
z
{\displaystyle x^{y^{z}}}
, the full exponent 
y
z
{\displaystyle y^{z}}
 of the base 
x
{\displaystyle x}
 is evaluated first. However, in some contexts, especially in handwriting, the difference between 
x
y
z
=
(
x
y
)
z
{\displaystyle {x^{y}}^{z}=(x^{y})^{z}}
, 
x
y
z
=
x
(
y
z
)
{\displaystyle x^{yz}=x^{(yz)}}
 and 
x
y
z
=
x
(
y
z
)
{\displaystyle x^{y^{z}}=x^{(y^{z})}}
 can be hard to see. In such a case, right-associativity is usually implied.Using right-associative notation for these operations can be motivated by the Curry–Howard correspondence and by the currying isomorphism.Non-associative operations for which no conventional evaluation order is defined include the following.
(Compare material nonimplication in logic.)William Rowan Hamilton seems to have coined the term "associative property"[17] around 1844, a time when he was contemplating the non-associative algebra of the octonions he had learned about from John T. Graves.[18]
Information box
{'Type': 'Law, rule of replacement', 'Field': 'Elementary algebra\nBoolean algebra\nSet theory\nLinear algebra\nPropositional calculus', 'Symbolic statement': 'Elementary algebra\nPropositional calculus'}
