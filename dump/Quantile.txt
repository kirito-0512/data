25184,
Quantile,
In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created. Common quantiles have special names, such as quartiles (four groups), deciles (ten groups), and percentiles (100 groups).  The groups created are termed halves, thirds, quarters, etc., though sometimes the terms for the quantile are used for the groups created, rather than for the cut points.
q-quantiles are values that partition a finite set of values into q subsets of (nearly) equal sizes.  There are q − 1 partitions of the q-quantiles, one for each integer k satisfying 0 < k < q.  In some cases the value of a quantile may not be uniquely determined, as can be the case for the median (2-quantile) of a uniform probability distribution on a set of even size.  Quantiles can also be applied to continuous distributions, providing a way to generalize rank statistics to continuous variables (see percentile rank).  When the cumulative distribution function of a random variable is known, the q-quantiles are the application of the quantile function (the inverse function of the cumulative distribution function) to the values {1/q, 2/q, …, (q − 1)/q}.
Some q-quantiles have special names:[citation needed]
As in the computation of, for example, standard deviation, the estimation of a quantile depends upon whether one is operating with a statistical population or with a sample drawn from it.  For a population, of discrete values or for a continuous population density, the k-th q-quantile is the data value where the cumulative distribution function crosses k/q.  That is, x is a k-th q-quantile for a variable X if
and
For a finite population of N equally probable values indexed 1, …, N from lowest to highest, the k-th q-quantile of this population can equivalently be computed via the value of Ip = N k/q.  If Ip is not an integer, then round up to the next integer to get the appropriate index; the corresponding data value is the k-th q-quantile.  On the other hand, if Ip is an integer then any number from the data value at that index to the data value of the next index can be taken as the quantile, and it is conventional (though arbitrary) to take the average of those two values (see Estimating quantiles from a sample).
If, instead of using integers k and q, the "p-quantile" is based on a real number p with 0 < p < 1 then p replaces k/q in the above formulas.  This broader terminology is used when quantiles are used to parameterize continuous probability distributions.  Moreover, some software programs (including Microsoft Excel) regard the minimum and maximum as the 0th and 100th percentile, respectively. However, this broader terminology is an extension beyond traditional statistics definitions.
The following two examples use the Nearest Rank definition of quantile with rounding. For an explanation of this definition, see percentiles.
Consider an ordered population of 10 data values [3, 6, 7, 8, 8, 10, 13, 15, 16, 20]. What are the 4-quantiles (the "quartiles") of this dataset?
So the first, second and third 4-quantiles (the "quartiles") of the dataset [3, 6, 7, 8, 8, 10, 13, 15, 16, 20]  are [7, 9, 15]. If also required, the zeroth quartile is 3 and the fourth quartile is 20.
Consider an ordered population of 11 data values [3, 6, 7, 8, 8, 9, 10, 13, 15, 16, 20]. What are the 4-quantiles (the "quartiles") of this dataset?
So the first, second and third 4-quantiles (the "quartiles") of the dataset [3, 6, 7, 8, 8, 9, 10, 13, 15, 16, 20]  are [7, 9, 15]. If also required, the zeroth quartile is 3 and the fourth quartile is 20.
For any population probability distribution on finitely many values, and generally for any probability distribution with a mean and variance, it is the case that




μ
−
σ
⋅




1
−
p

p



≤
Q
(
p
)
≤
μ
+
σ
⋅



p

1
−
p





,


{\displaystyle \mu -\sigma \cdot {\sqrt {\frac {1-p}{p}}}\leq Q(p)\leq \mu +\sigma \cdot {\sqrt {\frac {p}{1-p}}}\,,}


where Q(p) is the value of the p-quantile for 0 < p < 1 (or equivalently is the  k-th q-quantile for p = k/q), where μ is the distribution's arithmetic mean, and where σ is the distribution's standard deviation.[2]  In particular, the median (p = k/q = 1/2) is never more than one standard deviation from the mean.
The above formula can be used to bound the value μ + zσ in terms of quantiles.
When z ≥ 0, the value that is z standard deviations above the mean has a lower bound




μ
+
z
σ
≥
Q

(



z

2



1
+

z

2





)


,

 
f
o
r
 

z
≥
0.


{\displaystyle \mu +z\sigma \geq Q\left({\frac {z^{2}}{1+z^{2}}}\right)\,,\mathrm {~for~} z\geq 0.}


For example, the value that is z = 1 standard deviation above the mean is always greater than or equal to Q(p = 0.5), the median, and the value that is z = 2 standard deviations above the mean is always greater than or equal to Q(p = 0.8), the fourth quintile.
When z ≤ 0, there is instead an upper bound




μ
+
z
σ
≤
Q

(


1

1
+

z

2





)


,

 
f
o
r
 

z
≤
0.


{\displaystyle \mu +z\sigma \leq Q\left({\frac {1}{1+z^{2}}}\right)\,,\mathrm {~for~} z\leq 0.}


For example, the value μ + zσ for z = −3 will never exceed Q(p = 0.1), the first decile.
One problem which frequently arises is estimating a quantile of a (very large or infinite) population based on a finite sample of size N.
The asymptotic distribution of the p-th sample quantile is well-known: it is asymptotically normal around the p-th population quantile with variance equal to
where f(xp) is the value of the distribution density at the p-th population quantile (




x

p


=

F

−
1


(
p
)


{\displaystyle x_{p}=F^{-1}(p)}

).[3]
However, this distribution relies on knowledge of the population distribution; which is equivalent to knowledge of the population quantiles, which we are trying to estimate!  Modern statistical packages thus rely on a different technique — or selection of techniques — to estimate the quantiles.
Hyndman and Fan compiled a taxonomy of nine algorithms[4] used by various software packages.  
All methods compute Qp, the estimate for the p-quantile (the k-th q-quantile, where p = k/q) from a sample of size N by computing a real valued index h.  When h is an integer, the h-th smallest of the N values, xh, is the quantile estimate.  Otherwise a rounding or interpolation scheme is used to compute the quantile estimate from h, x⌊h⌋, and x⌈h⌉.  (For notation, see floor and ceiling functions).
The first three are piecewise constant, changing abruptly at each data point, while the last six use linear interpolation between data points, and differ only in how the index h used to choose the point along the piecewise linear interpolation curve, is chosen.
Mathematica,[5] Matlab,[6] R[7] and GNU Octave[8] programming languages support all nine sample quantile methods. SAS includes five sample quantile methods, SciPy[9] and Maple[10] both include eight, EViews[11] and Julia[12] include the six piecewise linear functions, Stata[13]  includes two, Python[14] includes two, and Microsoft Excel includes two. Mathematica, SciPy and Julia support arbitrary parameters for methods which allow for other, non-standard, methods.
The estimate types and interpolation schemes used include:
Notes:
Of the techniques, Hyndman and Fan recommend R-8, but most statistical software packages have chosen R-6 or R-7 as the default.[15]
The standard error of a quantile estimate can in general be estimated via the bootstrap.  The Maritz–Jarrett method can also be used.[16]
Computing approximate quantiles from data arriving from a stream can be done efficiently using compressed data structures. The most popular methods are t-digest[17] and KLL.[18] These methods read a stream of values in a continuous fashion and can, at any time, be queried about the approximate value of a specified quantile.
Both algorithms are based on a similar idea: compressing the stream of values by summarizing identical or similar values with a weight. If the stream is made of a repetition of 100 times v1 and 100 times v2, there is no reason to keep a sorted list of 200 elements, it is enough to keep two elements and two counts to be able to recover the quantiles. With more values, these algorithms maintain a trade-off between the number of unique values stored and the precision of the resulting quantiles. Some values may be discarded from the stream and contribute to the weight of a nearby value without changing the quantile results too much. The t-digest maintains a data structure of bounded size using an approach motivated by k-means clustering to group similar values. The KLL algorithm uses a more sophisticated "compactor" method that leads to better control of the error bounds at the cost of requiring an unbounded size if errors must be bounded relative to p.
Both methods belong to the family of data sketches that are subsets of Streaming Algorithms with useful properties: t-digest or KLL sketches can be combined. Computing the sketch for a very large vector of values can be split into trivially parallel processes where sketches are computed for partitions of the vector in parallel and merged later.
Standardized test results are commonly reported as a student scoring "in the 80th percentile", for example. This uses an alternative meaning of the word percentile as the interval between (in this case) the 80th and the 81st scalar percentile.[19] This separate meaning of percentile is also used in peer-reviewed scientific research articles.[20] The meaning used can be derived from its context.
If a distribution is symmetric, then the median is the mean (so long as the latter exists). But, in general, the median and the mean can differ. For instance, with a random variable that has an exponential distribution, any particular sample of this random variable will have roughly a 63% chance of being less than the mean. This is because the exponential distribution has a long tail for positive values but is zero for negative numbers.
Quantiles are useful measures because they are less susceptible than means to long-tailed distributions and outliers. Empirically, if the data being analyzed are not actually distributed according to an assumed distribution, or if there are other potential sources for outliers that are far removed from the mean, then quantiles may be more useful descriptive statistics than means and other moment-related statistics.
Closely related is the subject of least absolute deviations, a method of regression that is more robust to outliers than is least squares, in which the sum of the absolute value of the observed errors is used in place of the squared error. The connection is that the mean is the single estimate of a distribution that minimizes expected squared error while the median minimizes expected absolute error. Least absolute deviations shares the ability to be relatively insensitive to large deviations in outlying observations, although even better methods of robust regression are available.
The quantiles of a random variable are preserved under increasing transformations, in the sense that, for example, if m is the median of a random variable X, then 2m is the median of 2X, unless an arbitrary choice has been made from a range of values to specify a particular quantile. (See quantile estimation, above, for examples of such interpolation.) Quantiles can also be used in cases where only ordinal data are available.


