10236,
ELIZAeffect,
The ELIZA effect, in computer science, is the tendency to unconsciously assume computer behaviors are analogous to human behaviors; that is, anthropomorphisation.
It is named after a 1966 chatbot developed by MIT computer scientist Joseph Weizenbaum. This discovery has significantly influenced the development of artificial intelligence by demonstrating the potential of social engineering in passing a Turing test.
The phenomenon also has implications for automated labor. Chatbots, acting as personal assistants or specialized digital assistants, often mimic human behaviors to enhance productivity. Weizenbaum noted that emotional involvement with machines develops when we interact with them as if they were human.
Furthermore, the ELIZA effect underscores the complexity of human-machine interactions in the context of social and gender dynamics. Anthropomorphized chatbots often reflect gender stereotypes, influencing our relationship with technology and potentially reinforcing gender biases, especially when it comes to automated feminized labor.
The effect is named for ELIZA, the 1966 chatbot, developed by MIT computer scientist Joseph Weizenbaum. When executing Weizenbaum's DOCTOR script, ELIZA parodied a Rogerian psychotherapist, largely by rephrasing the "patient"'s replies as questions:[1]
Though designed strictly as a mechanism to support "natural language conversation" with a computer,[2] ELIZA's DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program's output.[3] As Weizenbaum later wrote, "I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people."[4] Indeed, ELIZA's code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA's questions implied interest and emotional involvement in the topics discussed, even when they consciously knew that ELIZA did not simulate emotion.[5]
In its specific form, the ELIZA effect refers only to "the susceptibility of people to read far more understanding than is warranted into strings of symbols—especially words—strung together by computers".[6] A trivial example of the specific form of the Eliza effect, given by Douglas Hofstadter, involves an automated teller machine which displays the words "THANK YOU" at the end of a transaction. A naive observer might think that the machine is actually expressing gratitude; however, the machine is only printing a preprogrammed string of symbols.[6]
More generally, the ELIZA effect describes any situation[7][8] where, based solely on a system's output, users perceive computer systems as having "intrinsic qualities and abilities which the software controlling the (output) cannot possibly achieve"[9] or "assume that [outputs] reflect a greater causality than they actually do".[10] In both its specific and general forms, the ELIZA effect is notable for occurring even when users of the system are aware of the determinate nature of output produced by the system.
From a psychological standpoint, the ELIZA effect is the result of a subtle cognitive dissonance between the user's awareness of programming limitations and their behavior towards the output of the program.[11]
The discovery of the ELIZA effect was an important development in artificial intelligence, demonstrating the principle of using social engineering rather than explicit programming to pass a Turing test.[12]
ELIZA convinced some users into thinking that a machine was human. This shift in human-machine interaction marked progress in technologies emulating human behavior. Two groups of chatbots are distinguished by William Meisel as "general personal assistants" and "specialized digital assistants".[13] General digital assistants have been integrated into personal devices, with skills like sending messages, taking notes, checking calendars, and setting appointments. Specialized digital assistants "operate in very specific domains or help with very specific tasks".[13] Digital assistants that are programmed to aid productivity by assuming behaviors analogous to humans.
Weizenbaum considered that not every part of the human thought could be reduced to logical formalisms and that "there are some acts of thought that ought to be attempted only by humans".[14] He also observed that we develop emotional involvement with machines if we interact with them as humans. When chatbots are anthropomorphized, they tend to portray gendered features as a way through which we establish relationships with the technology. "Gender stereotypes are instrumentalised to manage our relationship with chatbots" when human behavior is programmed into machines.[15]
In the 1990s, Clifford Nass and Byron Reeves conducted a series of experiments establishing The Media Equation, demonstrating that people tend to respond to media as they would either to another person (by being polite, cooperative, attributing personality characteristics such as aggressiveness, humor, expertise, and gender) – or to places and phenomena in the physical world. Numerous subsequent studies that have evolved from the research in psychology, social science and other fields indicate that this type of reaction is automatic, unavoidable, and happens more often than people realize. Reeves and Nass (1996) argue that, "Individuals' interactions with computers, television, and new media are fundamentally social and natural, just like interactions in real life," (p. 5).
Feminized labor, or women's work, automated by anthropomorphic digital assistants reinforces an "assumption that women possess a natural affinity for service work and emotional labour".[16] In defining our proximity to digital assistants through their human attributes, chatbots become gendered entities.


